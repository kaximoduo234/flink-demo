package com.flink.paimon.demo;

import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.TableResult;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.types.Row;
import org.apache.flink.util.CloseableIterator;
import org.junit.jupiter.api.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.testcontainers.containers.KafkaContainer;
import org.testcontainers.containers.MySQLContainer;
import org.testcontainers.junit.jupiter.Container;
import org.testcontainers.junit.jupiter.Testcontainers;
import org.testcontainers.utility.DockerImageName;

import java.util.ArrayList;
import java.util.List;

import static org.junit.jupiter.api.Assertions.*;

/**
 * ğŸ”¥ Paimoné›†æˆæµ‹è¯•å¥—ä»¶
 * 
 * æµ‹è¯•è¦†ç›–:
 * - Paimonè¡¨åˆ›å»ºå’Œç®¡ç†
 * - æ•°æ®æ’å…¥å’Œæµå¼æŸ¥è¯¢
 * - æ‰¹å¤„ç†æŸ¥è¯¢æ¨¡å¼
 * - Schemaæ¼”è¿›
 * - æ€§èƒ½åŸºå‡†æµ‹è¯•
 */
@Testcontainers
@TestMethodOrder(MethodOrderer.OrderAnnotation.class)
@DisplayName("ğŸ”¥ Paimon Integration Test Suite")
public class PaimonIntegrationTest {

    private static final Logger LOG = LoggerFactory.getLogger(PaimonIntegrationTest.class);
    
    private StreamExecutionEnvironment env;
    private StreamTableEnvironment tableEnv;

    @Container
    static KafkaContainer kafka = new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:7.4.0"))
            .withEmbeddedZookeeper()
            .withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "true");

    @Container
    static MySQLContainer<?> mysql = new MySQLContainer<>("mysql:8.0")
            .withDatabaseName("test_db")
            .withUsername("test")
            .withPassword("test");

    @BeforeEach
    void setUp() {
        LOG.info("ğŸš€ Setting up Flink environment...");
        
        Configuration config = new Configuration();
        config.setString("execution.checkpointing.interval", "10s");
        config.setString("execution.checkpointing.mode", "EXACTLY_ONCE");
        config.setString("state.backend", "rocksdb");
        config.setString("state.checkpoints.dir", "file:///tmp/checkpoints");
        
        env = StreamExecutionEnvironment.getExecutionEnvironment(config);
        env.setParallelism(1);
        
        tableEnv = StreamTableEnvironment.create(env);
        
        // é…ç½®Paimon Catalog
        tableEnv.executeSql("CREATE CATALOG paimon_catalog WITH (" +
                "'type' = 'paimon'," +
                "'warehouse' = 'file:///tmp/paimon-warehouse'" +
                ")");
        
        tableEnv.executeSql("USE CATALOG paimon_catalog");
        
        LOG.info("âœ… Flink environment initialized successfully");
    }
    
    @AfterEach
    void tearDown() {
        LOG.info("ğŸ§¹ Cleaning up test resources...");
        // æ¸…ç†æµ‹è¯•æ•°æ®
        try {
            tableEnv.executeSql("DROP TABLE IF EXISTS test_table");
        } catch (Exception e) {
            LOG.warn("Failed to drop test table: {}", e.getMessage());
        }
    }

    @Test
    @Order(1)
    @DisplayName("ğŸ”¥ Test Paimon Table Creation")
    void testPaimonTableCreation() {
        LOG.info("ğŸ§ª Testing Paimon table creation...");
        
        // åˆ›å»ºPaimonè¡¨
        TableResult result = tableEnv.executeSql("CREATE TABLE test_table (" +
                "id BIGINT," +
                "name STRING," +
                "age INT," +
                "score DOUBLE," +
                "create_time TIMESTAMP(3)," +
                "PRIMARY KEY (id) NOT ENFORCED" +
                ") WITH (" +
                "'bucket' = '2'," +
                "'changelog-producer' = 'input'" +
                ")");
        
        assertNotNull(result, "Table creation should return non-null result");
        LOG.info("âœ… Paimon table created successfully");
    }

    @Test
    @Order(2)
    @DisplayName("ğŸ”¥ Test Data Insertion and Streaming Query")
    void testDataInsertionAndQuery() throws Exception {
        LOG.info("ğŸ§ª Testing data insertion and streaming query...");
        
        // åˆ›å»ºæµ‹è¯•è¡¨
        tableEnv.executeSql("CREATE TABLE test_table (" +
                "id BIGINT," +
                "name STRING," +
                "age INT," +
                "score DOUBLE," +
                "create_time TIMESTAMP(3)," +
                "PRIMARY KEY (id) NOT ENFORCED" +
                ") WITH (" +
                "'bucket' = '2'," +
                "'changelog-producer' = 'input'" +
                ")");
        
        // æ’å…¥æµ‹è¯•æ•°æ®
        tableEnv.executeSql("INSERT INTO test_table VALUES " +
                "(1, 'Alice', 25, 95.5, TIMESTAMP '2024-01-01 10:00:00')," +
                "(2, 'Bob', 30, 88.0, TIMESTAMP '2024-01-01 11:00:00')," +
                "(3, 'Charlie', 35, 92.5, TIMESTAMP '2024-01-01 12:00:00')");
        
        // éªŒè¯æ•°æ®æ’å…¥
        TableResult queryResult = tableEnv.executeSql("SELECT COUNT(*) as cnt FROM test_table");
        
        try (CloseableIterator<Row> iterator = queryResult.collect()) {
            assertTrue(iterator.hasNext(), "Query should return results");
            Row row = iterator.next();
            long count = row.getFieldAs(0);
            assertEquals(3L, count, "Should have 3 records");
            LOG.info("âœ… Data insertion verified: {} records", count);
        }
    }

    @Test
    @Order(3)
    @DisplayName("ğŸ”¥ Test Batch Query Mode")
    void testBatchQueryMode() throws Exception {
        LOG.info("ğŸ§ª Testing batch query mode...");
        
        // åˆ›å»ºæ–°çš„æ‰¹å¤„ç†æ¨¡å¼ç¯å¢ƒï¼ˆä¸èƒ½åŠ¨æ€åˆ‡æ¢ï¼‰
        Configuration batchConfig = new Configuration();
        batchConfig.setString("execution.runtime-mode", "BATCH");
        StreamExecutionEnvironment batchEnv = StreamExecutionEnvironment.getExecutionEnvironment(batchConfig);
        StreamTableEnvironment batchTableEnv = StreamTableEnvironment.create(batchEnv);
        
        // é…ç½® Paimon Catalog
        batchTableEnv.executeSql("CREATE CATALOG paimon_catalog WITH (" +
                "'type' = 'paimon'," +
                "'warehouse' = 'file:///tmp/paimon-warehouse'" +
                ")");
        batchTableEnv.useCatalog("paimon_catalog");
        
        // åˆ›å»ºæµ‹è¯•è¡¨
        batchTableEnv.executeSql("CREATE TABLE batch_test_table (" +
                "id BIGINT," +
                "name STRING," +
                "amount DECIMAL(10,2)," +
                "PRIMARY KEY (id) NOT ENFORCED" +
                ") WITH (" +
                "'bucket' = '1'" +
                ")");
        
        // æ’å…¥æ‰¹é‡æ•°æ®
        batchTableEnv.executeSql("INSERT INTO batch_test_table VALUES " +
                "(1, 'Product A', 100.50)," +
                "(2, 'Product B', 200.75)," +
                "(3, 'Product C', 150.25)," +
                "(4, 'Product D', 300.00)");
        
        // æ‰§è¡ŒèšåˆæŸ¥è¯¢
        TableResult result = batchTableEnv.executeSql("SELECT " +
                "COUNT(*) as total_products," +
                "SUM(amount) as total_amount," +
                "AVG(amount) as avg_amount," +
                "MAX(amount) as max_amount " +
                "FROM batch_test_table");
        
        try (CloseableIterator<Row> iterator = result.collect()) {
            assertTrue(iterator.hasNext(), "Batch query should return results");
            Row row = iterator.next();
            
            assertEquals(Long.valueOf(4), row.getFieldAs(0), "Should have 4 products");
            assertEquals(751.50, ((Number) row.getFieldAs(1)).doubleValue(), 0.01, "Total amount should be 751.50");
            assertEquals(187.875, ((Number) row.getFieldAs(2)).doubleValue(), 0.001, "Average amount should be 187.875");
            assertEquals(300.00, ((Number) row.getFieldAs(3)).doubleValue(), 0.01, "Max amount should be 300.00");
            
            LOG.info("âœ… Batch query validation passed");
        }
    }

    @Test
    @Order(4)
    @DisplayName("ğŸ”¥ Test Schema Evolution")
    void testSchemaEvolution() throws Exception {
        LOG.info("ğŸ§ª Testing schema evolution...");
        
        // åˆ›å»ºåˆå§‹è¡¨
        tableEnv.executeSql("CREATE TABLE evolution_table (" +
                "id BIGINT," +
                "name STRING," +
                "PRIMARY KEY (id) NOT ENFORCED" +
                ")");
        
        // æ’å…¥åˆå§‹æ•°æ®
        tableEnv.executeSql("INSERT INTO evolution_table VALUES (1, 'Initial')");
        
        // æ·»åŠ æ–°åˆ—ï¼ˆæ¨¡æ‹Ÿschema evolutionï¼‰
        tableEnv.executeSql("ALTER TABLE evolution_table ADD age INT");
        
        // æ’å…¥åŒ…å«æ–°åˆ—çš„æ•°æ®
        tableEnv.executeSql("INSERT INTO evolution_table VALUES (2, 'Updated', 25)");
        
        // éªŒè¯schema evolution
        TableResult result = tableEnv.executeSql("SELECT * FROM evolution_table ORDER BY id");
        
        List<Row> rows = new ArrayList<>();
        try (CloseableIterator<Row> iterator = result.collect()) {
            while (iterator.hasNext()) {
                rows.add(iterator.next());
            }
        }
        
        assertEquals(2, rows.size(), "Should have 2 records");
        assertEquals("Initial", rows.get(0).getFieldAs("name"));
        assertEquals("Updated", rows.get(1).getFieldAs("name"));
        assertEquals(Integer.valueOf(25), rows.get(1).getFieldAs("age"));
        
        LOG.info("âœ… Schema evolution test passed");
    }

    @Test
    @Order(5)
    @DisplayName("ğŸ”¥ Test Performance Benchmark")
    void testPerformanceBenchmark() throws Exception {
        LOG.info("ğŸ§ª Running performance benchmark...");
        
        // åˆ›å»ºæ€§èƒ½æµ‹è¯•è¡¨
        tableEnv.executeSql("CREATE TABLE perf_table (" +
                "id BIGINT," +
                "data STRING," +
                "timestamp_col TIMESTAMP(3)," +
                "PRIMARY KEY (id) NOT ENFORCED" +
                ") WITH (" +
                "'bucket' = '4'," +
                "'compaction.min.file-num' = '5'," +
                "'compaction.max.file-num' = '10'" +
                ")");
        
        // æ€§èƒ½æµ‹è¯•ï¼šæ‰¹é‡æ’å…¥
        long startTime = System.currentTimeMillis();
        
        StringBuilder insertSql = new StringBuilder("INSERT INTO perf_table VALUES ");
        for (int i = 1; i <= 1000; i++) {
            if (i > 1) insertSql.append(",");
            insertSql.append("(").append(i).append(", 'data_").append(i).append("', TIMESTAMP '2024-01-01 10:00:00')");
        }
        
        tableEnv.executeSql(insertSql.toString());
        
        long insertTime = System.currentTimeMillis() - startTime;
        LOG.info("ğŸ“Š Batch insert time: {} ms for 1000 records", insertTime);
        
        // æ€§èƒ½æµ‹è¯•ï¼šæŸ¥è¯¢
        startTime = System.currentTimeMillis();
        TableResult queryResult = tableEnv.executeSql("SELECT COUNT(*) FROM perf_table WHERE id > 500");
        
        try (CloseableIterator<Row> iterator = queryResult.collect()) {
            assertTrue(iterator.hasNext(), "Performance query should return results");
            Row row = iterator.next();
            long count = row.getFieldAs(0);
            assertEquals(500L, count, "Should have 500 records with id > 500");
        }
        
        long queryTime = System.currentTimeMillis() - startTime;
        LOG.info("ğŸ“Š Query time: {} ms for filtering 1000 records", queryTime);
        
        // æ€§èƒ½æ–­è¨€ï¼šç¡®ä¿æ“ä½œåœ¨åˆç†æ—¶é—´å†…å®Œæˆ
        assertTrue(insertTime < 30000, "Batch insert should complete within 30 seconds");
        assertTrue(queryTime < 10000, "Query should complete within 10 seconds");
        
        LOG.info("âœ… Performance benchmark passed");
    }
} 